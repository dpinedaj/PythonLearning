{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Operations Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window Operations\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time\n",
    "DEMO: Do a demo exmaple: https://drive.google.com/open?id=0Bym8DZ5hyGifU2YzUmx3aldVdkU\n",
    "EXERCISE: Prepare an excise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "package com.sundogsoftware.sparkstreaming\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import java.util.regex.Pattern\n",
    "import java.util.regex.Matcher\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Locale\n",
    "\n",
    "import Utilities._\n",
    "\n",
    "object StructuredStreaming {\n",
    "  \n",
    "   // Case class defining structured data for a line of Apache access log data\n",
    "   case class LogEntry(ip:String, client:String, user:String, dateTime:String, request:String, status:String, bytes:String, referer:String, agent:String)\n",
    "   \n",
    "   val logPattern = apacheLogPattern()\n",
    "   val datePattern = Pattern.compile(\"\\\\[(.*?) .+]\")\n",
    "      \n",
    "   // Function to convert Apache log times to what Spark/SQL expects\n",
    "   def parseDateField(field: String): Option[String] = {\n",
    "\n",
    "      val dateMatcher = datePattern.matcher(field)\n",
    "      if (dateMatcher.find) {\n",
    "              val dateString = dateMatcher.group(1)\n",
    "              val dateFormat = new SimpleDateFormat(\"dd/MMM/yyyy:HH:mm:ss\", Locale.ENGLISH)\n",
    "              val date = (dateFormat.parse(dateString))\n",
    "              val timestamp = new java.sql.Timestamp(date.getTime());\n",
    "              return Option(timestamp.toString())\n",
    "          } else {\n",
    "          None\n",
    "      }\n",
    "   }\n",
    "   \n",
    "   // Convert a raw line of Apache access log data to a structured LogEntry object (or None if line is corrupt)\n",
    "   def parseLog(x:Row) : Option[LogEntry] = {\n",
    "     \n",
    "     val matcher:Matcher = logPattern.matcher(x.getString(0)); \n",
    "     if (matcher.matches()) {\n",
    "       val timeString = matcher.group(4)\n",
    "       return Some(LogEntry(\n",
    "           matcher.group(1),\n",
    "           matcher.group(2),\n",
    "           matcher.group(3),\n",
    "           parseDateField(matcher.group(4)).getOrElse(\"\"),\n",
    "           matcher.group(5),\n",
    "           matcher.group(6),\n",
    "           matcher.group(7),\n",
    "           matcher.group(8),\n",
    "           matcher.group(9)\n",
    "           ))\n",
    "     } else {\n",
    "       return None\n",
    "     }\n",
    "   }\n",
    "  \n",
    "   def main(args: Array[String]) {\n",
    "      // Use new SparkSession interface in Spark 2.0\n",
    "      val spark = SparkSession\n",
    "        .builder\n",
    "        .appName(\"StructuredStreaming\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\") // Necessary to work around a Windows bug in Spark 2.0.0; omit if you're not on Windows.\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", \"file:///C:/checkpoint\")\n",
    "        .getOrCreate()\n",
    "        \n",
    "      setupLogging()\n",
    "        \n",
    "      // Create a stream of text files dumped into the logs directory\n",
    "      val rawData = spark.readStream.text(\"logs\")\n",
    "            \n",
    "      // Must import spark.implicits for conversion to DataSet to work!\n",
    "      import spark.implicits._\n",
    "            \n",
    "      // Convert our raw text into a DataSet of LogEntry rows, then just select the two columns we care about\n",
    "      val structuredData = rawData.flatMap(parseLog).select(\"status\", \"dateTime\")\n",
    "    \n",
    "      // Group by status code, with a one-hour window.\n",
    "      val windowed = structuredData.groupBy($\"status\", window($\"dateTime\", \"1 hour\")).count().orderBy(\"window\")\n",
    "      \n",
    "      // Start the streaming query, dumping results to the console. Use \"complete\" output mode because we are aggregating\n",
    "      // (instead of \"append\").\n",
    "      val query = windowed.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "      \n",
    "      // Keep going until we're stopped.\n",
    "      query.awaitTermination()\n",
    "      \n",
    "      spark.stop()\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

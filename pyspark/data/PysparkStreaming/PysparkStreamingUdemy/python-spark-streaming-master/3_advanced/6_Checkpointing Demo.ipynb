{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible, Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.\n",
    "* _Metadata checkpointing_ - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:\n",
    "    * _Configuration_ - The configuration that was used to create the streaming application.\n",
    "    * _DStream operations_ - The set of DStream operations that define the streaming application.\n",
    "    * _Incomplete batches_ - Batches whose jobs are queued but have not completed yet.\n",
    "* _Data checkpointing_ - Saving of the generated RDDs to reliable storage. This is necessary in some _stateful_ transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically _checkpointed_ to reliable storage (e.g. HDFS) to cut off the dependency chains.\n",
    "To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used.\n",
    "\n",
    "### When to enable Checkpointing\n",
    "\n",
    "Checkpointing must be enabled for applications with any of the following requirements:\n",
    "* _Usage of stateful transformations_ - If either updateStateByKey or reduceByKeyAndWindow (with inverse function) is used in the application, then the checkpoint directory must be provided to allow for periodic RDD checkpointing.\n",
    "* _Recovering from failures of the driver running the application_ - Metadata checkpoints are used to recover with progress information.\n",
    "Note that simple streaming applications without the aforementioned stateful transformations can be run without enabling checkpointing. The recovery from driver failures will also be partial in that case (some received but unprocessed data may be lost). This is often acceptable and many run Spark Streaming applications in this way. Support for non-Hadoop environments is expected to improve in the future.\n",
    "\n",
    "### to configure Checkpointing\n",
    "\n",
    "Checkpointing can be enabled by setting a directory in a fault-tolerant, reliable file system (e.g., HDFS, S3, etc.) to which the checkpoint information will be saved. This is done by using streamingContext.checkpoint(checkpointDirectory). This will allow you to use the aforementioned stateful transformations. Additionally, if you want to make the application recover from driver failures, you should rewrite your streaming application to have the following behavior.\n",
    "\n",
    "* When the program is being started for the first time, it will create a new StreamingContext, set up all the streams and then call start().\n",
    "* When the program is being restarted after failure, it will re-create a StreamingContext from the checkpoint data in the checkpoint directory.\n",
    "\n",
    "This behavior is made simple by using StreamingContext.getOrCreate. This is used as follows.\n",
    "\n",
    "```python\n",
    "# Function to create and setup a new StreamingContext\n",
    "def functionToCreateContext():\n",
    "    sc = SparkContext(...)  # new context\n",
    "    ssc = StreamingContext(...)\n",
    "    lines = ssc.socketTextStream(...)  # create DStreams\n",
    "    ...\n",
    "    ssc.checkpoint(checkpointDirectory)  # set checkpoint directory\n",
    "    return ssc\n",
    "\n",
    "# Get StreamingContext from checkpoint data or create a new one\n",
    "context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)\n",
    "\n",
    "# Do additional setup on context that needs to be done,\n",
    "# irrespective of whether it is being started or restarted\n",
    "context. ...\n",
    "\n",
    "# Start the context\n",
    "context.start()\n",
    "context.awaitTermination()\n",
    "```\n",
    "If the checkpointDirectory exists, then the context will be recreated from the checkpoint data. If the directory does not exist (i.e., running for the first time), then the function functionToCreateContext will be called to create a new context and set up the DStreams. See the Python example recoverable_network_wordcount.py. This example appends the word counts of network data into a file.\n",
    "\n",
    "You can also explicitly create a StreamingContext from the checkpoint data and start the computation by using StreamingContext.getOrCreate(checkpointDirectory, None).\n",
    "\n",
    "In addition to using getOrCreate one also needs to ensure that the driver process gets restarted automatically on failure. This can only be done by the deployment infrastructure that is used to run the application. This is further discussed in the Deployment section.\n",
    "\n",
    "Note that checkpointing of RDDs incurs the cost of saving to reliable storage. This may cause an increase in the processing time of those batches where RDDs get checkpointed. Hence, the interval of checkpointing needs to be set carefully. At small batch sizes (say 1 second), checkpointing every batch may significantly reduce operation throughput. Conversely, checkpointing too infrequently causes the lineage and task sizes to grow, which may have detrimental effects. For stateful transformations that require RDD checkpointing, the default interval is a multiple of the batch interval that is at least 10 seconds. It can be set by using dstream.checkpoint(checkpointInterval). Typically, a checkpoint interval of 5 - 10 sliding intervals of a DStream is a good setting to try.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createContext(host, port, outputPath):\n",
    "    # If you do not see this printed, that means the StreamingContext has been loaded\n",
    "    # from the new checkpoint\n",
    "    print(\"Creating new context\")\n",
    "    if os.path.exists(outputPath):\n",
    "        os.remove(outputPath)\n",
    "    sc = SparkContext(appName=\"PythonStreamingCheckpointedWordCount\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    # Create a socket stream on target ip:port and count the\n",
    "    # words in input stream of \\n delimited text (eg. generated by 'nc')\n",
    "    lines = ssc.socketTextStream(host, port)\n",
    "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "    wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    def echo(time, rdd):\n",
    "        counts = \"Counts at time %s %s\" % (time, rdd.collect())\n",
    "        print(counts)\n",
    "        print(\"Appending to \" + os.path.abspath(outputPath))\n",
    "        with open(outputPath, 'a') as f:\n",
    "            f.write(counts + \"\\n\")\n",
    "\n",
    "    wordCounts.foreachRDD(echo)\n",
    "    return ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new context\n"
     ]
    }
   ],
   "source": [
    "host = 'localhost'\n",
    "port = 8880\n",
    "# TODO: Change this path to reflect your own directory  if you want to run this demo\n",
    "checkpoint = \"checkpoint\"\n",
    "output = \"output/checkdemo.txt\"\n",
    "\n",
    "ssc = StreamingContext.getOrCreate(checkpoint, lambda: createContext(host, int(port), output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts at time 2018-03-02 03:29:16 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:17 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:18 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:19 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:20 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:21 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:22 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:23 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:24 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:25 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:26 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:27 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:28 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:29 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:30 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:31 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:32 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:33 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:34 [('these', 1), ('was', 2), ('myself', 1), ('lie', 1), ('close', 1), ('but', 1), ('of', 8), ('than', 1), ('throw', 1), ('When,', 1), ('gleams', 1), ('impenetrable', 1), ('stray', 1), ('down', 1), ('trickling', 1), ('this', 1), ('lovely', 1), ('feel', 2), ('at', 1), ('as', 1), ('around', 1), ('like', 2), ('existence,', 1), ('valley', 1), ('artist', 1), ('wonderful', 1), ('spot,', 1), ('created', 1), ('neglect', 1), ('soul,', 1), ('sun', 1), ('incapable', 1), ('now.', 1), ('strikes', 1), ('single', 1), ('happy,', 1), ('stream;', 1), ('in', 2), ('sanctuary,', 1), ('alone,', 1), ('never', 1), ('sweet', 1), ('mornings', 1), ('souls', 1), ('grass', 1), ('entire', 1), ('moment;', 1), ('vapour', 1), ('steal', 1), ('yet', 1), ('absorbed', 1), ('possession', 1), ('drawing', 1), ('existence', 1), ('am', 2), ('into', 1), ('while', 1), ('greater', 1), ('which', 2), ('few', 1), ('for', 1), ('to', 1), ('with', 2), ('exquisite', 1), ('mere', 1), ('serenity', 1), ('and,', 1), ('has', 1), ('bliss', 1), ('friend,', 1), ('charm', 1), ('so', 2), ('whole', 1), ('a', 3), ('sense', 1), ('should', 1), ('trees,', 1), ('inner', 1), ('that', 2), ('stroke', 1), ('dear', 1), ('mine.', 1), ('A', 1), ('foliage', 1), ('by', 1), ('I', 9), ('enjoy', 1), ('among', 1), ('spring', 1), ('heart.', 1), ('talents.', 1), ('me,', 1), ('present', 1), ('tall', 1), ('meridian', 1), ('surface', 1), ('taken', 1), ('and', 4), ('tranquil', 1), ('teems', 1), ('my', 5), ('be', 1), ('upper', 1), ('the', 11)]\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:35 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:36 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:37 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:38 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:39 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:40 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:41 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:42 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:43 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:44 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:45 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:46 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts at time 2018-03-02 03:29:47 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n",
      "Counts at time 2018-03-02 03:29:48 []\n",
      "Appending to /home/matthew/pyspark-streaming/3_advanced/output/checkdemo.txt\n"
     ]
    }
   ],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

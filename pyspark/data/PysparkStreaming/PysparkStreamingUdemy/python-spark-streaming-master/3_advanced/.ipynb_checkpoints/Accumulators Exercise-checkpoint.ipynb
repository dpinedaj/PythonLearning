{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Accumulators](https://spark.apache.org/docs/latest/programming-guide.html#accumulators) and [Broadcast variables](https://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) cannot be recovered from checkpoint in Spark Streaming. If you enable checkpointing and use [Accumulators](https://spark.apache.org/docs/latest/programming-guide.html#accumulators) or [Broadcast variables](https://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) as well, youâ€™ll have to create lazily instantiated singleton instances for [Accumulators](https://spark.apache.org/docs/latest/programming-guide.html#accumulators) and [Broadcast variables](https://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) so that they can be re-instantiated after the driver restarts on failure. This is shown in the following example.\n",
    "```python\n",
    "def getWordBlacklist(sparkContext):\n",
    "    if (\"wordBlacklist\" not in globals()):\n",
    "        globals()[\"wordBlacklist\"] = sparkContext.broadcast([\"a\", \"b\", \"c\"])\n",
    "    return globals()[\"wordBlacklist\"]\n",
    "\n",
    "def getDroppedWordsCounter(sparkContext):\n",
    "    if (\"droppedWordsCounter\" not in globals()):\n",
    "        globals()[\"droppedWordsCounter\"] = sparkContext.accumulator(0)\n",
    "    return globals()[\"droppedWordsCounter\"]\n",
    "\n",
    "def echo(time, rdd):\n",
    "    # Get or register the blacklist Broadcast\n",
    "    blacklist = getWordBlacklist(rdd.context)\n",
    "    # Get or register the droppedWordsCounter Accumulator\n",
    "    droppedWordsCounter = getDroppedWordsCounter(rdd.context)\n",
    "\n",
    "    # Use blacklist to drop words and use droppedWordsCounter to count them\n",
    "    def filterFunc(wordCount):\n",
    "        if wordCount[0] in blacklist.value:\n",
    "            droppedWordsCounter.add(wordCount[1])\n",
    "            False\n",
    "        else:\n",
    "            True\n",
    "\n",
    "    counts = \"Counts at time %s %s\" % (time, rdd.filter(filterFunc).collect())\n",
    "\n",
    "wordCounts.foreachRDD(echo)\n",
    "```\n",
    "See the full [source code](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/streaming/recoverable_network_wordcount.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators\n",
    "What is Accumulators and usage of Accumulators\n",
    "DEMO: Do a demo with Accumulators\n",
    "EXERCISE: Give an Exercise with Accumulators\n",
    "Fault-tolerance\n",
    "https://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    " Counts words in text encoded with UTF8 received from the network every second.\n",
    " Usage: recoverable_network_wordcount.py <hostname> <port> <checkpoint-directory> <output-file>\n",
    "   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive\n",
    "   data. <checkpoint-directory> directory to HDFS-compatible file system which checkpoint data\n",
    "   <output-file> file to which the word counts will be appended\n",
    " To run this on your local machine, you need to first run a Netcat server\n",
    "    `$ nc -lk 9999`\n",
    " and then run the example\n",
    "    `$ bin/spark-submit examples/src/main/python/streaming/recoverable_network_wordcount.py \\\n",
    "        localhost 9999 ~/checkpoint/ ~/out`\n",
    " If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create\n",
    " a new StreamingContext (will print \"Creating new context\" to the console). Otherwise, if\n",
    " checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from\n",
    " the checkpoint data.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "# Get or register a Broadcast variable\n",
    "def getWordBlacklist(sparkContext):\n",
    "    if ('wordBlacklist' not in globals()):\n",
    "        globals()['wordBlacklist'] = sparkContext.broadcast([\"a\", \"b\", \"c\"])\n",
    "    return globals()['wordBlacklist']\n",
    "\n",
    "\n",
    "# Get or register an Accumulator\n",
    "def getDroppedWordsCounter(sparkContext):\n",
    "    if ('droppedWordsCounter' not in globals()):\n",
    "        globals()['droppedWordsCounter'] = sparkContext.accumulator(0)\n",
    "    return globals()['droppedWordsCounter']\n",
    "\n",
    "\n",
    "def createContext(host, port, outputPath):\n",
    "    # If you do not see this printed, that means the StreamingContext has been loaded\n",
    "    # from the new checkpoint\n",
    "    print(\"Creating new context\")\n",
    "    if os.path.exists(outputPath):\n",
    "        os.remove(outputPath)\n",
    "    sc = SparkContext(appName=\"PythonStreamingRecoverableNetworkWordCount\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    # Create a socket stream on target ip:port and count the\n",
    "    # words in input stream of \\n delimited text (eg. generated by 'nc')\n",
    "    lines = ssc.socketTextStream(host, port)\n",
    "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "    wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    def echo(time, rdd):\n",
    "        # Get or register the blacklist Broadcast\n",
    "        blacklist = getWordBlacklist(rdd.context)\n",
    "        # Get or register the droppedWordsCounter Accumulator\n",
    "        droppedWordsCounter = getDroppedWordsCounter(rdd.context)\n",
    "\n",
    "        # Use blacklist to drop words and use droppedWordsCounter to count them\n",
    "        def filterFunc(wordCount):\n",
    "            if wordCount[0] in blacklist.value:\n",
    "                droppedWordsCounter.add(wordCount[1])\n",
    "                False\n",
    "            else:\n",
    "                True\n",
    "\n",
    "        counts = \"Counts at time %s %s\" % (time, rdd.filter(filterFunc).collect())\n",
    "        print(counts)\n",
    "        print(\"Dropped %d word(s) totally\" % droppedWordsCounter.value)\n",
    "        print(\"Appending to \" + os.path.abspath(outputPath))\n",
    "        with open(outputPath, 'a') as f:\n",
    "            f.write(counts + \"\\n\")\n",
    "\n",
    "    wordCounts.foreachRDD(echo)\n",
    "    return ssc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 5:\n",
    "        print(\"Usage: recoverable_network_wordcount.py <hostname> <port> \"\n",
    "              \"<checkpoint-directory> <output-file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    host, port, checkpoint, output = sys.argv[1:]\n",
    "    ssc = StreamingContext.getOrCreate(checkpoint,\n",
    "                                       lambda: createContext(host, int(port), output))\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#accumulators-broadcast-variables-and-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updateStateByKey or mapWithState Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updateStateByKey\n",
    "The `updateStateByKey` operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.\n",
    "1. Define the state - The state can be an arbitrary data type.\n",
    "2. Define the state update function - Specify with a function how to update the state using the previous state and the new values from an input stream.\n",
    "In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.\n",
    "\n",
    "Let’s illustrate this with an example. Say you want to maintain a running count of each word seen in a text data stream. Here, the running count is the state and it is an integer. We define the update function as:\n",
    "```python\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)  # add the new values with the previous running count to get the new count\n",
    "```\n",
    "This is applied on a DStream containing words (say, the `pairs` DStream containing `(word, 1)` pairs in the earlier [example](https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example)).\n",
    "```python\n",
    "runningCounts = pairs.updateStateByKey(updateFunction)\n",
    "```\n",
    "The update function will be called for each word, with `newValues` having a sequence of 1’s (from the (word, 1) pairs) and the `runningCount` having the previous count. For the complete Python code, take a look at the example [stateful_network_wordcount.py](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/streaming/stateful_network_wordcount.py).\n",
    "\n",
    "Note that using `updateStateByKey` requires the checkpoint directory to be configured, which is discussed in detail in the [checkpointing](https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing) section.\n",
    "\n",
    "### mapWithState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stateful transformation\n",
    "Transformations\n",
    "UpdateStateByKey\n",
    "mapWithState\n",
    "DEMO Do a demo with UpdateStateByKey or mapWithState\n",
    "Needs come up with a proper scenario to use  mapWithState or UpdateStateByKey, such as some web session data.\n",
    "EXERCISE: Prepare an exercise with UpdateStateByKey or mapWithState\n",
    "https://drive.google.com/file/d/0Bym8DZ5hyGifWTJkQW5laUdwRU0/view?usp=drive_web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "package com.sundogsoftware.sparkstreaming\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "import java.util.regex.Pattern\n",
    "import java.util.regex.Matcher\n",
    "\n",
    "import Utilities._\n",
    "\n",
    "/** An example of using a State object to keep persistent state information across a stream. \n",
    " *  In this case, we'll keep track of clickstreams on sessions tied together by IP addresses.\n",
    " */\n",
    "object Sessionizer {\n",
    "  \n",
    "  /** This \"case class\" lets us quickly define a complex type that contains a session length and a list of URL's visited,\n",
    "   *  which makes up the session data state that we want to preserve across a given session. The \"case class\" automatically\n",
    "   *  creates the constructor and accessors we want to use.\n",
    "   */\n",
    "  case class SessionData(val sessionLength: Long, var clickstream:List[String]);\n",
    "  \n",
    "  /** This function gets called as new data is streamed in, and maintains state across whatever key you define. In this case,\n",
    "   *  it expects to get an IP address as the key, a String as a URL (wrapped in an Option to handle exceptions), and \n",
    "   *  maintains state defined by our SessionData class defined above. Its output is new key, value pairs of IP address\n",
    "   *  and the updated SessionData that takes this new line into account.\n",
    "   */\n",
    "  def trackStateFunc(batchTime: Time, ip: String, url: Option[String], state: State[SessionData]): Option[(String, SessionData)] = {\n",
    "    // Extract the previous state passed in (using getOrElse to handle exceptions)\n",
    "    val previousState = state.getOption.getOrElse(SessionData(0, List()))\n",
    "    \n",
    "    // Create a new state that increments the session length by one, adds this URL to the clickstream, and clamps the clickstream \n",
    "    // list to 10 items\n",
    "    val newState = SessionData(previousState.sessionLength + 1L, (previousState.clickstream :+ url.getOrElse(\"empty\")).take(10))\n",
    "    \n",
    "    // Update our state with the new state.\n",
    "    state.update(newState)\n",
    "    \n",
    "    // Return a new key/value result.\n",
    "    Some((ip, newState))\n",
    "  }\n",
    "  \n",
    "  // Needed only on Windows:\n",
    "  System.setProperty(\"spark.sql.warehouse.dir\",\"file:///C:/temp\")\n",
    "  \n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    // Create the context with a 1 second batch size\n",
    "    val ssc = new StreamingContext(\"local[*]\", \"Sessionizer\", Seconds(1))\n",
    "    \n",
    "    setupLogging()\n",
    "    \n",
    "    // Construct a regular expression (regex) to extract fields from raw Apache log lines\n",
    "    val pattern = apacheLogPattern()\n",
    "\n",
    "    // We'll define our state using our trackStateFunc function above, and also specify a \n",
    "    // session timeout value of 30 minutes.\n",
    "    val stateSpec = StateSpec.function(trackStateFunc _).timeout(Minutes(30))\n",
    "                         \n",
    "    // Create a socket stream to read log data published via netcat on port 9999 locally\n",
    "    val lines = ssc.socketTextStream(\"127.0.0.1\", 9999, StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    \n",
    "    // Extract the (ip, url) we want from each log line\n",
    "    val requests = lines.map(x => {\n",
    "      val matcher:Matcher = pattern.matcher(x)\n",
    "      if (matcher.matches()) {\n",
    "        val ip = matcher.group(1)\n",
    "        val request = matcher.group(5)\n",
    "        val requestFields = request.toString().split(\" \")\n",
    "        val url = scala.util.Try(requestFields(1)) getOrElse \"[error]\"\n",
    "        (ip, url)\n",
    "      } else {\n",
    "        (\"error\", \"error\")\n",
    "      }\n",
    "    })\n",
    "    \n",
    "    // Now we will process this data through our StateSpec to update the stateful session data\n",
    "    // Note that our incoming RDD contains key/value pairs of ip/URL, and that what our\n",
    "    // trackStateFunc above expects as input.\n",
    "    val requestsWithState = requests.mapWithState(stateSpec)\n",
    "    \n",
    "    // And we'll take a snapshot of the current state so we can look at it.\n",
    "    val stateSnapshotStream = requestsWithState.stateSnapshots()  \n",
    " \n",
    "    // Process each RDD from each batch as it comes in\n",
    "    stateSnapshotStream.foreachRDD((rdd, time) => {\n",
    "\n",
    "      // We'll expose the state data as SparkSQL, but you could update some external DB\n",
    "      // in the real world.\n",
    "      \n",
    "      // Get the singleton instance of SQLContext\n",
    "      val sqlContext = SQLContextSingleton.getInstance(rdd.context)\n",
    "      import sqlContext.implicits._\n",
    "      \n",
    "      // Slightly different syntax here from our earlier SparkSQL example. toDF can take a list\n",
    "      // of column names, and if the number of columns matches what's in your RDD, it just works\n",
    "      // without having to use an intermediate case class to define your records.\n",
    "      // Our RDD contains key/value pairs of IP address to SessionData objects (the output from\n",
    "      // trackStateFunc), so we first split it into 3 columns using map().\n",
    "      val requestsDataFrame = rdd.map(x => (x._1, x._2.sessionLength, x._2.clickstream)).toDF(\"ip\", \"sessionLength\", \"clickstream\")\n",
    "\n",
    "      // Create a SQL table from this DataFrame\n",
    "      requestsDataFrame.createOrReplaceTempView(\"sessionData\")\n",
    "\n",
    "      // Dump out the results - you can do any SQL you want here.\n",
    "      val sessionsDataFrame =\n",
    "        sqlContext.sql(\"select * from sessionData\")\n",
    "      println(s\"========= $time =========\")\n",
    "      sessionsDataFrame.show()\n",
    "\n",
    "    })\n",
    "    \n",
    "    // Kick it off\n",
    "    ssc.checkpoint(\"C:/checkpoint/\")\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "  }\n",
    "}\n",
    "\n",
    "/** Lazily instantiated singleton instance of SQLContext \n",
    " *  (Straight from included examples in Spark)  */\n",
    "object SQLContextSingleton {\n",
    "\n",
    "  @transient  private var instance: SQLContext = _\n",
    "\n",
    "  def getInstance(sparkContext: SparkContext): SQLContext = {\n",
    "    if (instance == null) {\n",
    "      instance = new SQLContext(sparkContext)\n",
    "    }\n",
    "    instance\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Transformations Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DStreams Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use any of the functions above to return the largest key of every RDD in a DStream (not just the largest in the entire DStream).\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Return a new DStream by passing each element of the source DStream through a function func.    |\n",
    "| **flatMap**(func)\t| Similar to map, but each input item can be mapped to 0 or more output items.    |\n",
    "| **filter**(func)\t| Return a new DStream by selecting only the records of the source DStream on which func returns true.    |\n",
    "| **repartition**(numPartitions)\t| Changes the level of parallelism in this DStream by creating more or fewer partitions.    |\n",
    "| **union**(otherStream)\t| Return a new DStream that contains the union of the elements in the source DStream and otherDStream. |\n",
    "| **count**()\t| Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.  |\n",
    "| **reduce**(func)\t| Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using  a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "| **countByValue**()\t| When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "| **reduceByKey**(func, [numTasks])\t| When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "| **join**(otherStream, [numTasks])\t| When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "| **cogroup**(otherStream, [numTasks])\t| When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "\n",
    "\n",
    "If you look at the spark streaming documentation, you will also find the `transform(func)` and `updateStateByKey(func)`. We will discuss these later in the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonStreamingExercise\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the stream\n",
    "stream = ssc.queueStream([sc.parallelize([(1,\"a\"), (2,\"b\"),(1,\"c\"),(2,\"d\"),\n",
    "(1,\"e\"),(3,\"f\")],3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use any of the functions above, or some combinaion, to \n",
    "# return the largest key of every RDD in a DStream (not just the largest in the entire DStream). \n",
    "\n",
    "\n",
    "\n",
    "###### End of Exercise section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start() \n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

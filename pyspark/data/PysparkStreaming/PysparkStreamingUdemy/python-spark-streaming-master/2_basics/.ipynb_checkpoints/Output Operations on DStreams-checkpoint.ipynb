{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Operations on DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output operations allow DStreamâ€™s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:\n",
    "\n",
    "| Output Operation        | Meaning           |\n",
    "|-------------:|:------------- |\n",
    "| **pprint**()      | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. |\n",
    "| **saveAsTextFiles**(prefix, [suffix])     | Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n",
    "| **saveAsObjectFiles**(prefix, [suffix]) | Save this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\". This is not available in the Python API. |\n",
    "| **saveAsHadoopFiles**(prefix, [suffix])      | Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\". This is not available in the Python API. |\n",
    "| **foreachRDD**(func) | The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different output operation\n",
    "* Print\n",
    "* saveAsTextFiles\n",
    "* saveAsObjectFiles (not available in Python)\n",
    "* saveAsHadoopFiles (not available in Python)\n",
    "* foreachRDD\n",
    "\n",
    "DEMO: Demo how to save tweets to files\n",
    "* Example: https://drive.google.com/open?id=0Bym8DZ5hyGifaXgwWFQxdVQ4UzA\n",
    "* use foreachRDD and saveAsTextFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala Example\n",
    "```scala\n",
    "package com.sundogsoftware.sparkstreaming\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.twitter._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "import Utilities._\n",
    "\n",
    "/** Listens to a stream of tweets and saves them to disk. */\n",
    "object SaveTweets {\n",
    "  \n",
    "  /** Our main function where the action happens */\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    // Configure Twitter credentials using twitter.txt\n",
    "    setupTwitter()\n",
    "    \n",
    "    // Set up a Spark streaming context named \"SaveTweets\" that runs locally using\n",
    "    // all CPU cores and one-second batches of data\n",
    "    val ssc = new StreamingContext(\"local[*]\", \"SaveTweets\", Seconds(1))\n",
    "    \n",
    "    // Get rid of log spam (should be called after the context is set up)\n",
    "    setupLogging()\n",
    "\n",
    "    // Create a DStream from Twitter using our streaming context\n",
    "    val tweets = TwitterUtils.createStream(ssc, None)\n",
    "    \n",
    "    // Now extract the text of each status update into RDD's using map()\n",
    "    val statuses = tweets.map(status => status.getText())\n",
    "    \n",
    "    // Here's one way to just dump every partition of every stream to individual files:\n",
    "    //statuses.saveAsTextFiles(\"Tweets\", \"txt\")\n",
    "    \n",
    "    // But let's do it the hard way to get a bit more control.\n",
    "    \n",
    "    // Keep count of how many Tweets we've received so we can stop automatically\n",
    "    // (and not fill up your disk!)\n",
    "    var totalTweets:Long = 0\n",
    "        \n",
    "    statuses.foreachRDD((rdd, time) => {\n",
    "      // Don't bother with empty batches\n",
    "      if (rdd.count() > 0) {\n",
    "        // Combine each partition's results into a single RDD:\n",
    "        val repartitionedRDD = rdd.repartition(1).cache()\n",
    "        // And print out a directory with the results.\n",
    "        repartitionedRDD.saveAsTextFile(\"Tweets_\" + time.milliseconds.toString)\n",
    "        // Stop once we've collected 1000 tweets.\n",
    "        totalTweets += repartitionedRDD.count()\n",
    "        println(\"Tweet count: \" + totalTweets)\n",
    "        if (totalTweets > 1000) {\n",
    "          System.exit(0)\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    \n",
    "    // You can also write results into a database of your choosing, but we'll do that later.\n",
    "    \n",
    "    // Set a checkpoint directory, and kick it all off\n",
    "    ssc.checkpoint(\"C:/checkpoint/\")\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "  }  \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: How to save tweets to files\n",
    "use foreachRDD and saveAsTextFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://drive.google.com/open?id=0Bym8DZ5hyGifaXgwWFQxdVQ4UzA\n",
    "2. https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

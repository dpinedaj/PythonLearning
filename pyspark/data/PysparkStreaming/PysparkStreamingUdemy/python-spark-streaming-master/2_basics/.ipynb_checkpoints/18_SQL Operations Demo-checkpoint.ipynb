{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Operations Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of Spark is its support for SQL operations when it comes to streaming data. \n",
    "\n",
    "To do this, one first creates a `SparkSession` using the `SparkContext` that the `StreamingContext` is using. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we will be using Spark and SQL to analyze data from a DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from pyspark.sql.functions import regexp_replace, col, trim, lower, desc\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[2]\", appName=\"PysparkSQLNetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.textFileStream(\"data\")\n",
    "words = lines.flatMap(lambda line: re.split(' ', line.lower().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The section below is for converting RDDs of the words DStream to DataFrame and run SQL query\n",
    "def analyze(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        # words of more than three characters\n",
    "        rdd = rdd.filter(lambda x: len(x)>3)\n",
    "        rdd.collect()\n",
    "\n",
    "        # 1 count per word\n",
    "        rdd = rdd.map(lambda w:(w,1))\n",
    "        words_df = sqlContext.createDataFrame(rdd,['word','count'])\n",
    "        words_df.show()\n",
    "\n",
    "        # replace punctuation\n",
    "        df_transformed = words_df.select(lower(trim(regexp_replace(col('word'),r'[.,\\/#$%^&*()-_+=~!\"\\s]*',''))).alias('keywords'))\n",
    "\n",
    "        # Print the top 250 words\n",
    "        top_words = sqlContext.createDataFrame(df_transformed.groupby('keywords').count().sort(desc('count')).take(251))\n",
    "        top_words.pprint()\n",
    "    except:\n",
    "        pass\n",
    "words.foreachRDD(analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2018-02-13 04:29:55 =========\n",
      "========= 2018-02-13 04:29:56 =========\n",
      "========= 2018-02-13 04:29:57 =========\n",
      "========= 2018-02-13 04:29:58 =========\n",
      "========= 2018-02-13 04:29:59 =========\n",
      "========= 2018-02-13 04:30:00 =========\n",
      "========= 2018-02-13 04:30:01 =========\n",
      "========= 2018-02-13 04:30:02 =========\n",
      "========= 2018-02-13 04:30:03 =========\n",
      "========= 2018-02-13 04:30:04 =========\n",
      "========= 2018-02-13 04:30:05 =========\n",
      "========= 2018-02-13 04:30:06 =========\n",
      "========= 2018-02-13 04:30:07 =========\n",
      "========= 2018-02-13 04:30:08 =========\n",
      "========= 2018-02-13 04:30:09 =========\n",
      "========= 2018-02-13 04:30:10 =========\n",
      "========= 2018-02-13 04:30:11 =========\n",
      "========= 2018-02-13 04:30:12 =========\n",
      "========= 2018-02-13 04:30:13 =========\n",
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|    seuss|    1|\n",
      "|   shine.|    1|\n",
      "|    play.|    1|\n",
      "|    house|    1|\n",
      "|     that|    1|\n",
      "|    cold,|    1|\n",
      "|    cold,|    1|\n",
      "|     day.|    1|\n",
      "|    there|    1|\n",
      "|     with|    1|\n",
      "|   sally.|    1|\n",
      "|   there,|    1|\n",
      "|     two.|    1|\n",
      "|    said,|    1|\n",
      "|     \"how|    1|\n",
      "|     wish|    1|\n",
      "|something|    1|\n",
      "|     do!\"|    1|\n",
      "|     cold|    1|\n",
      "|     play|    1|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2018-02-13 04:30:14 =========\n",
      "========= 2018-02-13 04:30:15 =========\n",
      "========= 2018-02-13 04:30:16 =========\n",
      "========= 2018-02-13 04:30:17 =========\n",
      "========= 2018-02-13 04:30:18 =========\n",
      "========= 2018-02-13 04:30:19 =========\n",
      "========= 2018-02-13 04:30:20 =========\n",
      "========= 2018-02-13 04:30:21 =========\n",
      "========= 2018-02-13 04:30:22 =========\n",
      "========= 2018-02-13 04:30:23 =========\n",
      "========= 2018-02-13 04:30:24 =========\n",
      "========= 2018-02-13 04:30:25 =========\n",
      "========= 2018-02-13 04:30:26 =========\n",
      "========= 2018-02-13 04:30:27 =========\n",
      "========= 2018-02-13 04:30:28 =========\n",
      "========= 2018-02-13 04:30:29 =========\n",
      "========= 2018-02-13 04:30:30 =========\n",
      "========= 2018-02-13 04:30:31 =========\n",
      "========= 2018-02-13 04:30:32 =========\n",
      "========= 2018-02-13 04:30:33 =========\n",
      "========= 2018-02-13 04:30:34 =========\n",
      "========= 2018-02-13 04:30:35 =========\n",
      "========= 2018-02-13 04:30:36 =========\n",
      "========= 2018-02-13 04:30:37 =========\n",
      "========= 2018-02-13 04:30:38 =========\n",
      "========= 2018-02-13 04:30:39 =========\n",
      "========= 2018-02-13 04:30:40 =========\n",
      "========= 2018-02-13 04:30:41 =========\n",
      "========= 2018-02-13 04:30:42 =========\n",
      "========= 2018-02-13 04:30:43 =========\n",
      "========= 2018-02-13 04:30:44 =========\n",
      "========= 2018-02-13 04:30:45 =========\n",
      "========= 2018-02-13 04:30:46 =========\n",
      "========= 2018-02-13 04:30:47 =========\n",
      "========= 2018-02-13 04:30:48 =========\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations\n",
    "2. https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

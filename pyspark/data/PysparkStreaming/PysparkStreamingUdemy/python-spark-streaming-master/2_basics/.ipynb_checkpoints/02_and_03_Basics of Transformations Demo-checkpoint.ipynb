{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Transformations Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark Streaming, DStreams are treated very similarly to the RDDs that make them up. Like RDDs, there are a wide variety of data transformation options. \n",
    "\n",
    "Here are some examples of the transformations from the Spark documentation that might be useful for your purposes\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Return a new DStream by passing each element of the source DStream through a function func.    |\n",
    "| **flatMap**(func)\t| Similar to map, but each input item can be mapped to 0 or more output items.    |\n",
    "| **filter**(func)\t| Return a new DStream by selecting only the records of the source DStream on which func returns true.    |\n",
    "| **repartition**(numPartitions)\t| Changes the level of parallelism in this DStream by creating more or fewer partitions.    |\n",
    "| **union**(otherStream)\t| Return a new DStream that contains the union of the elements in the source DStream and otherDStream. |\n",
    "| **count**()\t| Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.  |\n",
    "| **reduce**(func)\t| Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using  a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "| **countByValue**()\t| When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "| **reduceByKey**(func, [numTasks])\t| When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "| **join**(otherStream, [numTasks])\t| When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "| **cogroup**(otherStream, [numTasks])\t| When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "\n",
    "\n",
    "If you look at the spark streaming documentation, you will also find the `transform(func)` and `updateStateByKey(func)`. We will discuss these later in the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're going to be demoing the map and flatmap functions with respect to DStreams. One important question is \"What is the difference between the two?\"\n",
    "\n",
    "`map`: It returns a new RDD by applying a function to each element of the RDD. Function in map can return only one item. Works with DStreams as well as RDDs\n",
    "\n",
    "`flatMap`: Similar to map, it returns a new RDD by applying  a function to each element of the RDD, but output is flattened.\n",
    "Also, function in flatMap can return a list of elements (0 or more). Works with DStreams as well as RDDs.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonStreamingTransformationDemo\")\n",
    "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice o/p is flattened out in a single list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's Another Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the list is flattened in the latter version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example, this time interacting with a file, which can often be useful for debugging code that interacts with full DStreams\n",
    "\n",
    "There is a text file `greetings.txt` with following lines:\n",
    "```\n",
    "Good Morning\n",
    "Good Evening\n",
    "Good Day\n",
    "Happy Birthday\n",
    "Happy New Year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"greetings.txt\")\n",
    "lines.map(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we went over the `map` and `flapmap` functions. We'll explore a few other options.\n",
    "\n",
    "Suppose we have a this example text from Dr Suess's _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[2]\", appName=\"DrSeussExample\")\n",
    "scc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile = scc.sparkContext.textFile(\"/home/matthew/pyspark-streaming/2_basics/data/DrSeuss.text\")\n",
    "wordspair = myFile.flatMap(lambda row: row.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda x,y : x + y)\n",
    "oldwordcount = wordspair.reduceByKey(lambda x,y : x + y)\n",
    "lines = scc.socketTextStream(\"192.168.56.101\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.streaming.dstream.DStream object at 0x7fde00ef82b0>\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose then that we want to get wordcounts for this. We can use the map function from before here. `map` returns a new RDD containing values created by applying the supplied function to each value in the original RDD Here we use a lambda function which replaces some common punctuation characters with spaces and convert to lower  case, producing a new RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts1 = lines.map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "wordcounts1top = wordcounts1.transform(lambda rdd: rdd.take(10))\n",
    "wordcounts1top.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flatMap function takes these input values and returns a new, flattened list. In this case, the lines are split into words and then each word becomes a separate value in the output RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts2 = wordcounts1.flatMap(lambda x: x.split())\n",
    "wordcounts2top = wordcounts2.transform(lambda rdd: rdd.take(10))\n",
    "wordcounts2.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect that the input RDD contains tuples of the form (key,value). Create a new RDD containing a tuple for each unique value of key in the input, where the value in the second position of the tuple is created by  applying the supplied lambda function to the values with the matching key in the input RDD Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD  will consist of a single tuple for each unique word in the data, where the word is stored at the first position  in the tuple and the word count is stored at the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts3 = wordcounts2.map(lambda x: (x, 1))\n",
    "wordcounts3top = wordcounts3.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts3.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts4 = wordcounts3.reduceByKey(lambda x,y:x+y)\n",
    "wordcounts4top = wordcounts4.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts4.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map a lambda function to the data which will swap over the first and second values in each tuple, now the word count appears in the first position and the word in the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts5 = wordcounts4.map(lambda x:(x[1],x[0]))\n",
    "wordcounts5top = wordcounts5.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts5.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we sort the input RDD by the key value (i.e., the value at the first position in each tuple). In this example the first position stores the word count so this will sort the words so that the most frequently occurring words occur first in the RDD. The ascending=False parameter results in a descending sort order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformedDStream' object has no attribute 'sortByKey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7b26eeb9e879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordcounts6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcounts5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwordcounts6top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcounts6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwordcounts6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformedDStream' object has no attribute 'sortByKey'"
     ]
    }
   ],
   "source": [
    "wordcounts6 = wordcounts5.sortByKey(ascending=False)\n",
    "wordcounts6top = wordcounts6.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts6.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Operations Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily use DataFrames and SQL operations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore this has to done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier [word count example](https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example) to generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.\n",
    "\n",
    "```python\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "...\n",
    "\n",
    "# DataFrame operations inside your streaming program\n",
    "\n",
    "words = ... # DStream of strings\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        wordCountsDataFrame.show()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "words.foreachRDD(process)\n",
    "```\n",
    "See the [full source code](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/streaming/sql_network_wordcount.py).\n",
    "\n",
    "You can also run SQL queries on tables defined on streaming data from a different thread (that is, asynchronous to the running StreamingContext). Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run. Otherwise the StreamingContext, which is unaware of the any asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to query the last batch, but your query can take 5 minutes to run, then call streamingContext.remember(Minutes(5)) (in Scala, or equivalent in other languages).\n",
    "\n",
    "See the [DataFrames and SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) guide to learn more about DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark program to read the house data from in/RealEstate.csv, group by location, aggregate the average price per SQ Ft and sort by average price per SQ Ft.\n",
    "\n",
    "The houses dataset contains a collection of recent real estate listings in San Luis Obispo county and around it. \n",
    "\n",
    "The dataset contains the following fields:\n",
    "1. MLS: Multiple listing service number for the house (unique ID).\n",
    "2. Location: city/town where the house is located. Most locations are in San Luis Obispo county and northern Santa Barbara county (Santa Maria Orcutt, Lompoc, Guadelupe, Los Alamos), but there some out of area locations as well.\n",
    "3. Price: the most recent listing price of the house (in dollars).\n",
    "4. Bedrooms: number of bedrooms.\n",
    "5. Bathrooms: number of bathrooms.\n",
    "6. Size: size of the house in square feet.\n",
    "7. Price/SQ.ft: price of the house per square foot.\n",
    "8. Status: type of sale. Thee types are represented in the dataset: Short Sale, Foreclosure and Regular.\n",
    "        \n",
    "Each field is comma separated.\n",
    "Sample output:\n",
    "```bash\n",
    "+----------------+-----------------+\n",
    "|        Location| avg(Price SQ Ft)|\n",
    "+----------------+-----------------+\n",
    "|          Oceano|             95.0|\n",
    "|         Bradley|            206.0|\n",
    "| San Luis Obispo|            359.0|\n",
    "|      Santa Ynez|            491.4|\n",
    "|         Cayucos|            887.0|\n",
    "|................|.................|\n",
    "|................|.................|\n",
    "|................|.................|\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Example\n",
    "```scala\n",
    "\n",
    "package com.sparkTutorial.sparkSql\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object HousePriceSolution {\n",
    "\n",
    "  val PRICE_SQ_FT = \"Price SQ Ft\"\n",
    "\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "    val session = SparkSession.builder().appName(\"HousePriceSolution\").master(\"local[1]\").getOrCreate()\n",
    "\n",
    "    val realEstate = session.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", value = true)\n",
    "      .csv(\"in/RealEstate.csv\")\n",
    "\n",
    "    realEstate.groupBy(\"Location\")\n",
    "      .avg(PRICE_SQ_FT)\n",
    "      .orderBy(\"avg(Price SQ Ft)\")\n",
    "      .show()\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
